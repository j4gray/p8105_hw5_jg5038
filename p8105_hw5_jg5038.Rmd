---
title: "p8105_hw5_jg5038"
author: "Julia Gray"
date: "2025-11-05"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

#these options were messing up my fig.height 
#knitr::opts_chunk$set(
#  fig.width = 6,
#  fig.asp = .6
#)

knitr::opts_chunk$set(
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1: Birthday problem

Create a function to simulate birhday sampling in a room with a set number of subjects (n_room):

```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_room

  repeated_bday
}

#bday_sim(20)
```

Run the simulation 1000 times for groups of size 5-50:

```{r}
bday_sim_results = 
  expand_grid(
    bdays = 5:50,
    iter = 1:1000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by (
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

Plot results:

```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0.5, color = "orange") +
  labs(
    title = "Probability of 2+ People Sharing a Birthday based on Group Size",
    x = "Group Size",
    y = "Probability"
  )
```

The probability of 2 people sharing a birthday is hits 50% when the group size is at least `r min(pull(filter(bday_sim_results, prob_repeat > 0.5), bdays))`.

### Problem 2

Set model params and create function:

```{r}
#set model params
n = 30
sigma = 5
mu = 0

n_trials = 5000
alpha = 0.05

#create simulation function
norm_dist_sim_fn = function (mu = 0, n = 30, sigma = 5, alpha = 0.05) {
  
#generate data
norm_vec = rnorm(n = n, mean = mu, sd = sigma)
  
#run ttest and return estimate & p.value
t.test(norm_vec, mu = 0, conf.level = 1-alpha) |> 
  broom::tidy() |> 
  select(estimate, p.value)
}
```

Run for mu = 0

```{r}
#test = norm_dist_sim_fn(mu=mu)

#run for mu = 0
norm_sim_results_0 = 
  expand_grid(
    mean = mu,
    iter = 1:n_trials
  ) |> 
  mutate(
    t_results = map(mean, norm_dist_sim_fn)
  ) |> 
  unnest(t_results)

```

Run for mu in {1, 2, 3, 4, 5, 6}

```{r}
#run for mu in {1, 2, 3, 4, 5, 6}
norm_sim_results = 
  expand_grid(
    mean = 1:6,
    iter = 1:n_trials
  ) |> 
  mutate(
    t_results = map(mean, norm_dist_sim_fn)
  ) |> 
  unnest(t_results)
```

Plot the results:

```{r}
norm_sim_results |> 
  group_by(mean) |> 
  summarize(
    power = sum(p.value < alpha) /  n()
  ) |> 
  ggplot(aes(x = mean, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Effect Size Vs. Power",
    x = "Effect Size (true mean)",
    y = "Power"
  )
```

We can see there is a positive correlation between effect size and power. When the effect size is small, the power is low meaning the frequency of null hypothesis getting rejected goes down. As the effect size goes up the power approaches 1, meaning the null hypothesis is more frequently getting rejected.


```{r}
avg_estimates = norm_sim_results |> 
  mutate(
    null_reject = (p.value < 0.05)
  )

all_sample_avg_estimates = avg_estimates |> 
  group_by(mean) |> 
  summarize(avg_estimate = mean(estimate), n = n()) |> 
  mutate(sample = 'all')

null_reject_avg_estimates = avg_estimates |> 
  filter(null_reject == TRUE) |> 
  group_by(mean) |> 
  summarize(avg_estimate = mean(estimate), n = n()) |> 
  mutate(sample = 'null_reject')

rbind(all_sample_avg_estimates, null_reject_avg_estimates) |> 
  ggplot(aes(x = mean, y = avg_estimate, color = sample)) + 
  geom_point() + 
  labs(
    color = "Source of Average Estimate",
    x = "True Mean",
    y = "Average Estimate"
  )
```

At larger effect sizes we can see that $\hat{\mu}$  is approximately equal to the true ${\mu}$. When the effect size is low, the power is low and we frequently fail to reject the null hypothesis. 

### Problem 3

```{r}
homicide_df = read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(
    reported_date = as.Date(as.character(reported_date), format = "%y%m%d"),
    city_state = paste(city, state, sep=", ")
  )
```

The Washington Post dataset on homicides contains `r dim(homicide_df)[1]` observations. Each observation is a homicide and contains columns for `r colnames(homicide_df)`. 

```{r}
homicide_df |> 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) |> 
  group_by(city_state) |> 
  summarize(unsolved_homicides = n()) |> 
  knitr::kable()
```

Prop test for Baltimore:

```{r}
prop_test_baltimore = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    n_total = n()
  ) |> 
  with(prop.test(x = n_unsolved, n = n_total))

prop_test_baltimore |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high) |> 
  knitr::kable()
```

Prop test for all cities:

```{r}
prop_test_all_cities = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize(
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    n_total = n()
  ) |> 
  mutate(
    prop_test = map2(n_unsolved, n_total, prop.test),
    results = map(prop_test, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(city_state, estimate, conf.low, conf.high)

prop_test_all_cities |> 
  knitr::kable(digits = 3)
```

Graph the results:

```{r fig.height=6.5}
prop_test_all_cities |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
    ) |> 
  ggplot(aes(y = city_state, x = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  xlim(0, 1) + 
  labs(
    title = "Proportion of Unsolved Homicides by City",
    y = "City",
    x = "Proportion Estimate (CI)"
  )
```










